{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd0ecae",
   "metadata": {},
   "source": [
    "# Assignment 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db263942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8511\n",
      "Test Accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('../datasets/adult.csv')  # Replace with your actual file path\n",
    "\n",
    "# Select features and target\n",
    "features = [\n",
    "    'age', 'workclass', 'education', 'marital-status', 'occupation',\n",
    "    'relationship', 'race', 'gender', 'capital-gain', 'capital-loss',\n",
    "    'hours-per-week', 'native-country'\n",
    "]\n",
    "\n",
    "target = 'income'\n",
    "\n",
    "# Drop rows with missing values in selected columns\n",
    "df = df.dropna(subset=features + [target])\n",
    "\n",
    "# One-Hot Encode categorical features\n",
    "X = pd.get_dummies(df[features])\n",
    "\n",
    "# Encode the target column if it's categorical\n",
    "y = df[target].map({'<=50K': 0, '>50K': 1})  # Adjust mapping if needed\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train (70%) and test (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and train the Perceptron (MLP with 1 hidden neuron)\n",
    "learning_rate = 0.01\n",
    "max_iter = 50\n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae897ee2",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Just like in assignment 20 the learning rate and max iterations have a big impact on the resulting accuracy, I found that a learning rate of .01 and a max_iter of 50 work wel for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
